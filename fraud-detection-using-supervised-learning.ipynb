{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Fraud Detection Using Supervised Learning**\n#### DTSA 5509 - Introduction to Machine Learning: Supervised Learning - Final Project ####\n<hr>\n<hr>\n\n> **1. Project Overview**\n> \n> **2. Project Setup**\n>\n> **3. Exploratory Data Analysis (EDA)**\n>\n> **4. Feature Selection**\n>\n> **5. Feature Engineering**\n>\n> **6. Classification Models**\n>\n>>\n>> *6.1 Logistic Regression*\n>>\n>> *6.2 Decision Tree*\n>>\n>> *6.3 Random Forest*\n>>\n>> *6.4 Naive Bayes*\n>>\n>> *6.5 Support Vector Machines (SVM)*\n>>\n> **7. Classification Performance**\n>\n> **8. Area Under the Precision-Recall Curve (AUPRC)**\n>\n> **9. Results and Conclusion**","metadata":{}},{"cell_type":"markdown","source":"## **1) Project Overview**\n\n### *About the Data:* ###\nThe dataset contains transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, with 492 cases of frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nThe dataset contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, neither the original features nor more background information about the data can be provided. Features `V1`, `V2`, … `V28` are the principal components obtained with PCA. \n\nThe only features which have not been transformed with PCA are `Time` and `Amount`. Feature `Time` contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature `Amount` is the transaction amount – this feature can be used for example-dependent cost-sensitive learning. Feature `Class` is the response variable which takes value 1 in case of fraud and 0 otherwise.\n\n### *Objective:* ###\nFind the most accurate supervised learning model for classifying fradulent credit card transcations.\n\n*Based on [this Kaggle dataset.](https://www.kaggle.com/datasets/whenamancodes/fraud-detection?datasetId=2472961&sortBy=voteCount)*\n<hr>","metadata":{}},{"cell_type":"markdown","source":"## **2) Project Setup**","metadata":{}},{"cell_type":"code","source":"# SUPPRESS WARNINGS\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IMPORT LIBRARIES\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import zscore\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport time\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, average_precision_score, precision_recall_curve","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SET NOTEBOOK OPTIONS\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [15, 15]\nnp.random.seed(31415)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# READ IN DATA\ndata = pd.read_csv('/kaggle/input/dtsa-5509-final-project/creditcard.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>","metadata":{}},{"cell_type":"markdown","source":"## **3. Exploratory Data Analysis (EDA)**\n<hr>","metadata":{}},{"cell_type":"code","source":"# Preview data\ndisplay(data)\n\n# Check data types\ndata.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All variables are the correct data type.<br>\n30 numerical features and 1 boolean variable (`Class`).","metadata":{}},{"cell_type":"code","source":"# Check for missing values\nany(data.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No missing values.","metadata":{}},{"cell_type":"code","source":"# Check for blank values\nnp.where(data.applymap(lambda x: x == ''))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No blank values.","metadata":{}},{"cell_type":"code","source":"# Count frequency of 'Class' labels\n# 0 = Not Fraud; 1 = Fraud\nprint('Non-Fraudulent Transactions:', len(data[data['Class'] == 0]))\nprint('Fraudulent Transactions:', len(data[data['Class'] == 1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'Class' labels countplot with percentages\ntotal = data.Class.count() # total count\n\nax = sns.countplot(x='Class', data=data)\nax.bar_label(ax.containers[0], fmt=lambda x: f'{(x/total)*100:0.2f}%')\nax.set(xlabel = 'Class', ylabel = 'Count')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only 0.17% of the data is fraud data.","metadata":{}},{"cell_type":"markdown","source":"## **4. Feature Selection**\n<hr>","metadata":{}},{"cell_type":"markdown","source":"### Outlier Detection\n<hr>\n\nReduce the number of features by selecting the most important features using the interquartile range (IQR)| method for outlier detection.<br>\nFeatures with a greater number of outliers that are also labelled as fraud *(`Class` = 1)* will be selected for the classification algorithms.\n<hr>","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# IQR outlier detection function\ndef detect_outliers(df):\n    \n    out_data = pd.DataFrame().reindex_like(df)\n    out_data = out_data.fillna(0)\n\n    df = df.apply(zscore)\n\n    features = list(df.columns.values)\n    n_features = len(features)\n\n    for col in range(n_features):\n        feature = df[features[col]]\n        q1 = np.percentile(feature, 25) # 1st quartile (25%)\n        q3 = np.percentile(feature, 75) # 3rd quartile (75%)\n        iqr = q3 - q1 # IQR\n\n        # Outlier Bounds\n        lower = (q1 - 1.5 * iqr)\n        upper = (q3 + 1.5 * iqr)\n        \n        # Consider any data point LESS than lower bound or GREATER than upper bound as an outlier\n        outlier_index = df[(df[features[col]] < lower) | (df[features[col]] > upper)].index # list of outlier indices for every features \n        out_data.loc[outlier_index, features[col]] = 1 # mark outlier values as 1, otherwise 0\n    \n    return out_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature dataframe\nfeatures = [col for col in data if col.startswith('V')] # select feature variables only\nfeature_data = data[features]\nfeature_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply IQR outlier detection function\nout_data = detect_outliers(feature_data)\nout_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get indices of rows labelled as fraud\nidx = data[data['Class']==1].index\n\n# Count number of fraud outliers for each feature\nout_count = out_data.iloc[idx,:].sum().sort_values()\nout_count = pd.DataFrame(data=out_count).T\n\nout_count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(out_count).set(xlabel='Feature', ylabel='Outlier Count')\nplt.yticks(np.arange(0, 500, 50))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get Q3 (75th percentile) of outlier counts for feature selection\nround(np.percentile(out_count, 75))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select features where number of outliers is greater than 314\nselected_features = out_count.columns[out_count.loc[0].gt(314)].tolist()\n\nselected_data = data[selected_features]\nselected_data['Class'] = data['Class'] # add in 'Class' variable\nselected_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Plots\n<hr>","metadata":{}},{"cell_type":"code","source":"# Distribution plots\nfig, axes = plt.subplots(4, 2, figsize=(15, 15))\nax = axes.flatten()\n\nfor i, col in enumerate(selected_features):\n    sns.histplot(selected_data[col], ax=ax[i], kde=True, element='poly').set_ylabel('', labelpad=0)\nfig.tight_layout(w_pad=6, h_pad=4)\nfig.delaxes(ax[7])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pairplot of 1000 rows, ordered by fraud cases\nsns.pairplot(selected_data.sort_values('Class', ascending=False).head(1000), hue='Class');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **5. Feature Engineering**\n<hr>\nStandardize features to have a mean of 0 and variance of 1.\n<hr>","metadata":{}},{"cell_type":"code","source":"standardizer = StandardScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardize features (X)\nX = selected_data.drop('Class', axis=1)\nX = standardizer.fit_transform(X)\nX","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target (y)\ny = selected_data['Class']\ny = np.array(y)\ny","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training and Testing Data\n<hr>","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **6. Classification Models**\n<hr>\n\n1. Logistic Regression\n\n2. Decision Tree\n\n3. Random Forest\n\n4. Naive Bayes\n\n5. Support Vector Machines (SVM)\n\n<hr>","metadata":{}},{"cell_type":"code","source":"models = {}\ntimes = {}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **6.1. Logistic Regression** ###\n<hr>\n\nLogistic regression uses the logistic function to classify the inputs into two categories and calculate the probability (between 0 and 1) of an input belonging to the default class (class 0).\n\nNew inputs are classified based on their calculated probabilities using a decision threshold of 0.5. If the probability is greater than 0.5, we can take the output as a prediction for the default class (class 0), otherwise the prediction is for the other class (class 1).\n<hr>","metadata":{}},{"cell_type":"code","source":"start_time = time.time() # code execution start time\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\n\nend_time = time.time() # code execution end time\n# print(\"Total execution time: {} seconds\".format(end_time - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models['Logistic Regression'] = y_pred_lr\ntimes['Logistic Regression'] = end_time - start_time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **6.2. Decision Tree** ###\n<hr>\n\nDecision tree classification recursively partitions the data based on input features to make predictions. The tree structure consists of nodes representing feature tests and branches representing possible outcomes. \n\nThe goal is to split the data in a way that maximizes the separation of classes (0 or 1) or minimizes prediction errors. New data is classified or predicted by following the path from the root to a leaf node, which provides the final output.\n\n<hr>","metadata":{}},{"cell_type":"code","source":"start_time = time.time() # code execution start time\n\n# dt = DecisionTreeClassifier(criterion='gini')\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ny_pred_dt = dt.predict(X_test)\n\nend_time = time.time() # code execution end time\n# print(\"Total execution time: {} seconds\".format(end_time - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models['Decision Tree'] = y_pred_dt\ntimes['Decision Tree'] = end_time - start_time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **6.3. Random Forest** ###\n<hr>\nRandom Forest classification assembles multiple decision trees to enhance predictive accuracy. It reduces overfitting by training each tree on different data subsets. The final prediction is a combination of outputs from individual trees, leading to more reliable and accurate results.\n<hr>","metadata":{}},{"cell_type":"code","source":"start_time = time.time() # code execution start time\n\n# rf = RandomForestClassifier(n_estimators=100, min_samples_split=2)\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\n\nend_time = time.time() # code execution end time\n# print(\"Total execution time: {} seconds\".format(end_time - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models['Random Forest'] = y_pred_rf\ntimes['Random Forest'] = end_time - start_time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **6.4. Naive Bayes** ###\n<hr>\nNaive Bayes is particularly effective for dealing with high-dimensional data because it assumes feature independent. and is based on Bayes' therom, which assumes that input features are independent of each other.\n\nThe algorithm calculates class probabilities for based on Bayes' theorem, and assigns the most probable class to an input.\n<hr>","metadata":{}},{"cell_type":"code","source":"start_time = time.time() # code execution start time\n\nnb = GaussianNB()\nnb.fit(X_train, y_train)\ny_pred_nb = nb.predict(X_test)\n\nend_time = time.time() # code execution end time\n# print(\"Total execution time: {} seconds\".format(end_time - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models['Naive Bayes'] = y_pred_nb\ntimes['Naive Bayes'] = end_time - start_time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **6.5. Support Vector Machines (SVM)** ###\n<hr>\n\nSupport Vector Machine (SVM) aims to find the optimal boundary that best separates the classes in the data (class 0 and 1) by identifying support vectors, which are data points closest to the decision boundary. \n\nBy utilizing kernel functions, SVM accommodates complex patterns in the data, enabling it to handle both linear and non-linear relationships.\n\n<hr>","metadata":{}},{"cell_type":"code","source":"start_time = time.time() # code execution start time\n\nsvm = SVC(kernel='rbf')\nsvm.fit(X_train, y_train)\ny_pred_svm = svm.predict(X_test)\n\nend_time = time.time() # code execution end time\n# print(\"Total execution time: {} seconds\".format(end_time - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models['Support Vector Machines'] = y_pred_svm\ntimes['Support Vector Machines'] = end_time - start_time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **7. Classification Performance**\n<hr>\nTo compare model performance we will look at the following metrics:\n\n1. **Accuracy:** The proportion of correctly classified instances over the total number of instances.\n\n2. **Precision:** The ratio of true positive predictions to the total predicted positives.\n\n3. **Recall:** Also known as Sensitivity or True Positive Rate, it's the ratio of true positive predictions to the total actual positives.\n\n4. **F1-Score:** A single value that balances a model's precision and recall, effectively summarizing its ability to correctly identify positive instances while minimizing false positives and false negatives.\n\n5. **Average Precision Score:** A metric used to evaluate the quality of binary classification models, measuring the average precision of positive class predictions across different levels of classification thresholds.\n\n6. **Runtime (seconds):** The amount of time (in seconds) an algorithm takes to complete its operations. It's a critical performance metric that indicates the efficiency and speed of a process or computation.\n\n<hr>","metadata":{}},{"cell_type":"code","source":"# Calculate performance metrics\naccuracy, precision, recall, f1score, avg_precision, runtime = [], [], [], [], [], []\n\nfor i in list(range(0, len(models))):\n    accuracy.append(accuracy_score(y_test, list(models.values())[i]))\n    precision.append(precision_score(y_test, list(models.values())[i]))\n    recall.append(recall_score(y_test, list(models.values())[i]))\n    f1score.append(f1_score(y_test, list(models.values())[i]))\n    avg_precision.append(average_precision_score(y_test, list(models.values())[i]))\n    runtime.append(list(times.values())[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Performance metrics dataframe\nmetrics = pd.DataFrame([accuracy, precision, recall, f1score, avg_precision, runtime]).T\nmetrics.columns = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Average Precision Score', 'Runtime']\nmetrics.index = models.keys()\nmetrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Performance metrics plot\nmetrics.drop('Runtime', axis=1).plot.bar(rot=360, color=['powderblue', 'salmon', 'rosybrown', 'lavender', 'palegoldenrod'])\nplt.legend(ncol= len(models.keys()), loc='upper center', fontsize=12)\nplt.tight_layout();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Highest Accuracy\nprint('Highest Accuracy:', metrics['Accuracy'].idxmax(), '= {:.4f}'.format(metrics['Accuracy'].max()))\n\n# Highest Precision\nprint('Highest Precision:', metrics['Precision'].idxmax(), '= {:.4f}'.format(metrics['Precision'].max()))\n\n# Highest Recall\nprint('Highest Recall:', metrics['Recall'].idxmax(), '= {:.4f}'.format(metrics['Recall'].max()))\n\n# Highest F1-Score\nprint('Highest F1-Score:', metrics['F1-Score'].idxmax(), '= {:.4f}'.format(metrics['F1-Score'].max()))\n\n# Highest Average Precision Score\nprint('Highest Average Precision Score:', metrics['Average Precision Score'].idxmax(), '= {:.4f}'.format(metrics['Average Precision Score'].max()))\n\n# Fastest Runtime \nprint('Fastest Runtime:', metrics['Runtime'].idxmax(), '= {:.4f} seconds'.format(metrics['Average Precision Score'].max()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **8. Area Under the Precision-Recall Curve (AUPRC)**\n<hr>\n\nGiven the class imbalance ratio in the data, we will also look at accuracy using the Area Under the Precision-Recall Curve (AUPRC).\n\nThe AUPRC evaluates the quality of binary classification models, and is a graphical representation of the trade-off between precision and recall as the classification threshold varies. A high AUPRC value indicates that the model maintains high precision across different levels of recall, making it effective at correctly identifying positive instances while minimizing false positives.\n\n<hr>","metadata":{}},{"cell_type":"code","source":"# Calculate AUPRC\nauprc = {}\n\nfor model_name in metrics.index:\n    precision = metrics.loc[model_name, 'Precision']\n    recall = metrics.loc[model_name, 'Recall']\n    auprc_value = auc(recall, precision) if isinstance(precision, list) else precision\n    auprc[model_name] = auprc_value\n    \nauprc    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot AUPRC\nfig, axes = plt.subplots(3, 2, figsize=(15, 15))\nax = axes.flatten()\n\nfor i in list(range(0, len(models))):\n    precision, recall, _ = precision_recall_curve(y_test, list(models.values())[i])\n    \n    ax[i].step(recall, precision, color='b', alpha=0.2, where='post')\n    ax[i].fill_between(recall, precision, alpha=0.2, color='b')\n    ax[i].set_xlabel('Recall')\n    ax[i].set_ylabel('Precision')\n    ax[i].set_title('Precision-Recall Curve: ' + list(models.keys())[i])\n    ax[i].text(0.2, 0.3, f'AUPRC = {list(auprc.values())[i]:.4f}', fontsize=12, color='red')\nfig.tight_layout(w_pad=6, h_pad=4)\nfig.delaxes(ax[5])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Highest AUPRC\nprint('Highest AUPRC:', max(auprc), '=', '{:.4f}'.format(max(auprc.values())))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **9. Results and Conclusion**\n<hr>\n\n### Results\nIn summary, we conducted a comprehensive comparison of five supervised classification models — Logistic Regression, Decision Tree, Random Forest, Naive Bayes, and Support Vector Machines (SVM) — for credit card fraud detection. The objective was to identify the most effective model for accurately identifying fraudulent transactions while minimizing false positives.\n\nThe performance of each model was evaluated using two critical metrics: the F1-score and the Area Under the Precision-Recall Curve (AUPRC).\n\nThe Random Forest model demonstrated the highest F1-score among the evaluated models (0.8603). This suggests that the Random Forest model strikes a balance between precision and recall, making it well-suited for cases where false positives and false negatives carry different consequences. It effectively manages to classify fraudulent transactions while minimizing errors in both directions.\n\nOn the other hand, the Support Vector Machines (SVM) model exhibited the highest AUPRC (0.9612). This indicates that the SVM model excels at correctly identifying and ranking fraudulent transactions with high precision. Given the focus on identifying the positive class and the imbalanced nature of fraud detection data, the SVM model's strong AUPRC performance is particularly noteworthy.\n\n\n### Conclusion\nIn the realm of credit card fraud detection, accurate identification of fraudulent transactions is of paramount importance, and precision is paramount. False positives can disrupt legitimate transactions and erode user trust. The SVM model's high AUPRC score signifies its ability to identify and rank fraudulent transactions with precision, mitigating the risk of false positives and highliting the model's capability to accurately classify rare instances, such as fraudulent transactions, by concentrating on the positive class and minimizing the chances of misclassification.\n\nSince credit card fraud data is inherently imbalanced, with a vast majority of transactions being legitimate, the SVM model's adeptness at handling such scenarios makes it a pragmatic choice. By emphasizing the positive class, it exhibits the ability to correctly identify rare instances of fraudulent activity in large and noisy data.\n\nIn conclusion, the Support Vector Machines (SVM) model stands as the optimal choice for credit card fraud detection. Its robust AUPRC performance, strategic approach to imbalanced data, and holistic solution underscore its suitability for a critical and dynamic domain.\n<hr>","metadata":{}},{"cell_type":"markdown","source":"<hr>\n<hr>\n\n#### [GitHub Project Repository](https://github.com/yevi7113/Supervised-Learning-Project/tree/main)","metadata":{}}]}